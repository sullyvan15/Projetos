# -*- coding: utf-8 -*-
"""sound_recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RZGKQPXdee-mVrns2XRm9xJtZEE9lqOF
"""

import librosa
import pandas as pd
import numpy as np
import os
import csv
import IPython
import random
import matplotlib.pyplot as plt
import librosa.display
from random import randint
from random import sample

import warnings
warnings.filterwarnings('ignore')

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.tree import ExtraTreeClassifier
from sklearn.metrics import accuracy_score

!wget https://github.com/sullyvan15/datasets/raw/master/audios.zip

!mkdir datasets ; mkdir datasets/audios
!mv audios.zip datasets/audios 
!cd datasets/audios ; unzip  audios.zip ; rm -rf audios.zip

import IPython

audio_file = './datasets/audios/homem/homem_(5).ogg'
IPython.display.Audio(audio_file)

import librosa

audio_data, sr = librosa.load(audio_file)

librosa.display.waveplot(audio_data)

"""## Comparar ondas sonoras

"""

homens = list()
mulheres = list()

for h in [1, 6, 13]:
    audio_data, _ = librosa.load('./datasets/audios/{0}/{0}_({1}).ogg'.format('homem', h))
    homens.append(audio_data)

for m in [11, 12, 16]:
    audio_data, _ = librosa.load('./datasets/audios/{0}/{0}_({1}).ogg'.format('mulher', m))
    mulheres.append(audio_data)

fig, axs = plt.subplots(3, 2, figsize=(15,10))

for i, audio_data in enumerate(homens):
    librosa.display.waveplot(audio_data, ax=axs[i, 0])
    
for i, audio_data in enumerate(mulheres):
    librosa.display.waveplot(audio_data, ax=axs[i, 1], color='orange')

"""## Spectograma"""

audio_file = './datasets/audios/mulher/mulher_(17).ogg'
audio_data, sr = librosa.load(audio_file)

X = librosa.stft(audio_data)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(20, 5))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')
plt.colorbar()

X = librosa.stft(audio_data)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(20, 5))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')
plt.colorbar()

"""## Comprando Spectogramas"""

fig, axs = plt.subplots(3, 2, figsize=(15,10))

def spec(audio_data, ax):
    X = librosa.stft(audio_data)
    Xdb = librosa.amplitude_to_db(abs(X))
    plt.figure(figsize=(20, 5))
    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log', ax=ax)

for i, audio_data in enumerate(homens):
    spec(audio_data, axs[i, 0])

for i, audio_data in enumerate(mulheres):
    spec(audio_data, axs[i, 1])

"""## Features Extration"""

files = list()
X = list()
y = list()


for label in ['homem', 'mulher']:
    for file in os.listdir('./datasets/audios/{}'.format(label)):
        audio_file = './datasets/audios/{}/{}'.format(label, file)
        audio_data, sr = librosa.load(audio_file, offset=0.5, duration=1)
        audio_stft = librosa.stft(audio_data)
        audio_db = librosa.amplitude_to_db(abs(audio_stft))
        files.append(audio_file)
        X.append(np.reshape(audio_db, audio_db.size))
        y.append(label)

scaler = MinMaxScaler((0, 1))
X_scaler = scaler.fit_transform(X)
X_best = SelectKBest(chi2, k=1000).fit_transform(X_scaler, y)

"""# Classificação"""

X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(X_best), y, test_size=0.3, random_state=42)

clf = ExtraTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy_score(y_test, y_pred)

conferencia = X_test.copy()
conferencia['y_pred'] = y_pred
conferencia['y_real'] = y_test
conferencia.join(pd.DataFrame(files, columns=['filename']))[['y_pred', 'y_real', 'filename']]

audio_file = './datasets/audios/{0}/{0}_({1}).ogg'.format('homem', 2)

audio_data, sr = librosa.load(audio_file, offset=0.5, duration=1)
X = librosa.stft(audio_data)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(20, 5))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')
plt.colorbar()

"""# Estrutura Final"""

from sklearn.base import BaseEstimator, TransformerMixin

class AudioDecode(BaseEstimator, TransformerMixin):
    def __main__(self):
        pass
    
    def __init__(self, offset=0.5, duration=1):
        self.offset = offset
        self.duration = duration

    def fit(self, X, y = None):
        self.X = X
        return self
    
    def transform(self, X, y = None):
        X_ = list()
        for index, value in X.iterrows():
            audio_data, sr = librosa.load(value[0], offset=self.offset, duration=self.duration)
            audio_stft = librosa.stft(audio_data)
            audio_db = librosa.amplitude_to_db(abs(audio_stft))
            X_.append(np.reshape(audio_db, audio_db.size))
        
        return pd.DataFrame(X_)

from sklearn.pipeline import Pipeline

pipe = Pipeline([('pre_process', AudioDecode(offset=0.5, duration=1)),
                 ('scaler', MinMaxScaler((0, 1))), 
                 ('feature_select', SelectKBest(chi2, k=1000)), 
                 ('classifier', ExtraTreeClassifier(random_state=42))])

X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(files), y, test_size=0.3, random_state=12)
pipe.fit(X_train, y_train)

y_pred = pipe.predict(X_test)
accuracy_score(y_test, y_pred)

!mkdir modelos

from joblib import dump, load

dump(pipe, 'modelos/sound_recognition.joblib')

modelo  = load('modelos/sound_recognition.joblib')

modelo.predict(X_test)

modelo.predict(pd.DataFrame(['./datasets/audios/mulher/mulher_(10).ogg']))[0]

X_test